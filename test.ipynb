{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from RLBrain_FunctionApproximation.BaseFunctionApproximation import feature_transform\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(1, 10, (1000, 2))\n",
    "y = np.sum(3.*x, axis=1) + np.random.random((1000, )) + np.sum(4 * x**2, axis=1)\n",
    "np.sum(3.*x, axis=1).shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 44.58, NNZs: 2, Bias: -30.872128, T: 1000, Avg. loss: 1799.977677\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 49.61, NNZs: 2, Bias: -50.295570, T: 2000, Avg. loss: 1286.601392\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 49.77, NNZs: 2, Bias: -65.076442, T: 3000, Avg. loss: 1072.004106\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 51.52, NNZs: 2, Bias: -76.497327, T: 4000, Avg. loss: 931.868136\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 51.95, NNZs: 2, Bias: -86.129286, T: 5000, Avg. loss: 834.182332\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 53.66, NNZs: 2, Bias: -93.893891, T: 6000, Avg. loss: 772.279361\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 55.80, NNZs: 2, Bias: -100.265175, T: 7000, Avg. loss: 715.942918\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 56.37, NNZs: 2, Bias: -105.830293, T: 8000, Avg. loss: 677.271162\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 57.96, NNZs: 2, Bias: -110.650044, T: 9000, Avg. loss: 657.776869\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 55.48, NNZs: 2, Bias: -115.182373, T: 10000, Avg. loss: 630.583293\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 56.94, NNZs: 2, Bias: -118.599675, T: 11000, Avg. loss: 612.764116\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 58.12, NNZs: 2, Bias: -121.721688, T: 12000, Avg. loss: 601.292008\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 57.53, NNZs: 2, Bias: -124.632474, T: 13000, Avg. loss: 589.299800\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 58.21, NNZs: 2, Bias: -127.122984, T: 14000, Avg. loss: 581.963726\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 57.58, NNZs: 2, Bias: -129.390299, T: 15000, Avg. loss: 574.162639\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 58.62, NNZs: 2, Bias: -131.279566, T: 16000, Avg. loss: 569.629212\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 59.57, NNZs: 2, Bias: -132.861645, T: 17000, Avg. loss: 563.993026\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 59.00, NNZs: 2, Bias: -134.492008, T: 18000, Avg. loss: 560.495360\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 58.97, NNZs: 2, Bias: -135.993155, T: 19000, Avg. loss: 559.972783\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 59.57, NNZs: 2, Bias: -137.173477, T: 20000, Avg. loss: 559.605744\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 59.95, NNZs: 2, Bias: -138.279910, T: 21000, Avg. loss: 552.791669\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 58.65, NNZs: 2, Bias: -139.451046, T: 22000, Avg. loss: 548.917321\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 59.20, NNZs: 2, Bias: -140.228631, T: 23000, Avg. loss: 539.938779\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 60.28, NNZs: 2, Bias: -140.925553, T: 24000, Avg. loss: 547.308505\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 60.21, NNZs: 2, Bias: -141.752526, T: 25000, Avg. loss: 551.059087\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 60.51, NNZs: 2, Bias: -142.332544, T: 26000, Avg. loss: 544.011384\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 59.39, NNZs: 2, Bias: -143.127466, T: 27000, Avg. loss: 547.266337\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 60.80, NNZs: 2, Bias: -143.535831, T: 28000, Avg. loss: 548.473881\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 28 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='squared_loss', max_iter=100,\n",
       "       n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD = SGDRegressor(max_iter=100, tol=.0001, verbose=1, warm_start=True)\n",
    "SGD.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 59.59, NNZs: 2, Bias: -145.137981, T: 1000, Avg. loss: 584.486786\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 60.36, NNZs: 2, Bias: -145.824176, T: 2000, Avg. loss: 560.601463\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 60.01, NNZs: 2, Bias: -146.592723, T: 3000, Avg. loss: 559.029575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 60.53, NNZs: 2, Bias: -146.947538, T: 4000, Avg. loss: 549.594045\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 60.64, NNZs: 2, Bias: -147.272076, T: 5000, Avg. loss: 552.813886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 60.68, NNZs: 2, Bias: -147.556440, T: 6000, Avg. loss: 551.130001\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 60.75, NNZs: 2, Bias: -147.877562, T: 7000, Avg. loss: 548.547739\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 60.26, NNZs: 2, Bias: -148.174687, T: 8000, Avg. loss: 543.797704\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 61.47, NNZs: 2, Bias: -148.123046, T: 9000, Avg. loss: 544.008742\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 61.07, NNZs: 2, Bias: -148.413016, T: 10000, Avg. loss: 548.299731\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 61.67, NNZs: 2, Bias: -148.534901, T: 11000, Avg. loss: 548.020723\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 61.64, NNZs: 2, Bias: -148.646534, T: 12000, Avg. loss: 545.741112\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 59.81, NNZs: 2, Bias: -148.957294, T: 13000, Avg. loss: 546.095813\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 13 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='squared_loss', max_iter=100,\n",
       "       n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        self.trans_type = 'polynomials'\n",
    "        self.max_iter = 10000\n",
    "        self.lr = 0.00001\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'GradientDescent(trans_type=%s, max_iter=%d, lr=%f)' %\\\n",
    "               (self.trans_type, self.max_iter, self.lr)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "    def _fit(self, x, y, verbs=False):\n",
    "        x_trans = feature_transform(x, trans_tpye=self.trans_type)\n",
    "        coef = self.coef_\n",
    "        if not coef:\n",
    "            coef = np.random.random((x_trans.shape[-1], 1))\n",
    "        cost = np.linalg.norm(y - np.dot(x_trans, coef), 2)\n",
    "        for i in range(self.max_iter):\n",
    "            for j in range(len(x_trans)):\n",
    "                x_j = x_trans[j].reshape(1, -1)\n",
    "                loss = (y[j] - np.dot(x_j, coef))[0]\n",
    "                coef += self.lr * loss * x_j.T\n",
    "            cost_i = np.linalg.norm(y - np.dot(x_trans, coef), 2)\n",
    "                            \n",
    "            if verbs:\n",
    "                if i % 10 == 0:\n",
    "                    cost_i = np.linalg.norm(y - np.dot(x_trans, coef), 2)\n",
    "                    print('iteration %d, mean square loss %f' % (i, cost_i))\n",
    "                    \n",
    "            if np.abs(cost_i - cost) < 0.001:\n",
    "                break\n",
    "            cost = cost_i\n",
    "        self.coef_ = coef\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def fit(self, x, y, verbs=False):\n",
    "        return self._fit(x, y, verbs=verbs)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        x_trans = feature_transform(x, trans_tpye=self.trans_type)\n",
    "\n",
    "        if self.coef_:\n",
    "            raise Exception('model has not been fitted!!!')\n",
    "        else:\n",
    "            return np.dot(x_trans, self.coef_)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self._predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(1, 10, (1000, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.sum(3.*x, axis=1) + np.random.random((1000, )) + np.sum(4 * x**2, axis=1)\n",
    "np.sum(3.*x, axis=1).shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, mean square loss 166494.747935\n",
      "iteration 10, mean square loss 160789.959184\n",
      "iteration 20, mean square loss 160162.069919\n",
      "iteration 30, mean square loss 159693.299685\n",
      "iteration 40, mean square loss 159327.955567\n",
      "iteration 50, mean square loss 159043.099048\n",
      "iteration 60, mean square loss 158821.018254\n",
      "iteration 70, mean square loss 158647.933662\n",
      "iteration 80, mean square loss 158513.126175\n",
      "iteration 90, mean square loss 158408.265358\n",
      "iteration 100, mean square loss 158326.892650\n",
      "iteration 110, mean square loss 158264.026339\n",
      "iteration 120, mean square loss 158215.861709\n",
      "iteration 130, mean square loss 158179.537894\n",
      "iteration 140, mean square loss 158152.921402\n",
      "iteration 150, mean square loss 158134.320120\n",
      "iteration 160, mean square loss 158122.109605\n",
      "iteration 170, mean square loss 158114.544682\n",
      "iteration 180, mean square loss 158109.996641\n",
      "iteration 190, mean square loss 158107.258275\n",
      "iteration 200, mean square loss 158105.574164\n",
      "iteration 210, mean square loss 158104.508021\n",
      "iteration 220, mean square loss 158103.812606\n",
      "iteration 230, mean square loss 158103.346153\n",
      "iteration 240, mean square loss 158103.025381\n",
      "iteration 250, mean square loss 158102.799968\n",
      "iteration 260, mean square loss 158102.638631\n",
      "iteration 270, mean square loss 158102.521380\n",
      "iteration 280, mean square loss 158102.435112\n",
      "iteration 290, mean square loss 158102.371024\n",
      "iteration 300, mean square loss 158102.323072\n",
      "iteration 310, mean square loss 158102.287022\n",
      "iteration 320, mean square loss 158102.259854\n",
      "iteration 330, mean square loss 158102.239376\n",
      "iteration 340, mean square loss 158102.223981\n",
      "iteration 350, mean square loss 158102.212473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientDescent(trans_type=polynomials, max_iter=10000, lr=0.000010)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = GradientDescent()\n",
    "lr.fit(x, y, verbs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.87079002],\n",
       "       [2.68302364],\n",
       "       [2.72162673],\n",
       "       [4.01963694],\n",
       "       [0.01947831],\n",
       "       [4.01530075]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(np.sqrt)\n",
    "np.linalg.norm([2, 1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
